{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0bd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StackedAE import Convolution_Auto_Encoder as CAE\n",
    "from StackedAE import Convolution_AE_Classification as CAC\n",
    "import torch\n",
    "import torch.nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(\"\"))))\n",
    "import commons\n",
    "import utils\n",
    "import audio_processing as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf2de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/caijb/data_drive/autoencoder/log/kernel5/G_203.pth\n",
      "INFO:root:Loaded checkpoint '/media/caijb/data_drive/autoencoder/log/kernel5/G_203.pth' (iteration 203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Convolution_Auto_Encoder(\n",
       "   (encoder): Encoder(\n",
       "     (conv2d_layer_1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (conv2d_layer_2): Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (relu): ReLU()\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (Tconv2d_layer1): ConvTranspose2d(5, 3, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (Tconv2d_layer2): ConvTranspose2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (relu): ReLU()\n",
       "   )\n",
       " ),\n",
       " None,\n",
       " 0.0001,\n",
       " 203)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model load from save path\n",
    "config_path =  \"/media/caijb/data_drive/autoencoder/log/kernel5/config.json\"\n",
    "\n",
    "#load paramters\n",
    "with open(config_path, 'r') as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "hps = utils.HParams(**config)\n",
    "\n",
    "#load model & load saved model weight\n",
    "checkpoint_path = utils.latest_checkpoint_path(hps.train.model_dir)\n",
    "\n",
    "model = CAE(encoder_dim=hps.model.encoder_dim, hidden_1dim=hps.model.hidden_dim1,\n",
    "    hidden_2dim=hps.model.hidden_dim2, kernel=hps.model.kernel)\n",
    "\n",
    "utils.load_checkpoint(checkpoint_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c3bede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.conv2d_layer_1.weight', 'encoder.conv2d_layer_1.bias', 'encoder.conv2d_layer_2.weight', 'encoder.conv2d_layer_2.bias', 'decoder.Tconv2d_layer1.weight', 'decoder.Tconv2d_layer1.bias', 'decoder.Tconv2d_layer2.weight', 'decoder.Tconv2d_layer2.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51636ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor= torch.randn(1,80,430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29486d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 72, 422])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=model.get_vector(tensor)\n",
    "encoder.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d514e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = CAC(encoder_dim=hps.model.encoder_dim, hidden_1dim=hps.model.hidden_dim1,\n",
    "    hidden_2dim=hps.model.hidden_dim2, kernel=hps.model.kernel, hps = hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c56e0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.conv2d_layer_1.weight', 'encoder.conv2d_layer_1.bias', 'encoder.conv2d_layer_2.weight', 'encoder.conv2d_layer_2.bias', 'classification.conv1x1.weight', 'classification.conv1x1.bias', 'classification.linear1.weight', 'classification.linear1.bias', 'classification.linear2.weight', 'classification.linear2.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dbffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0990, -0.1630,  0.0887,  0.1558,  0.0999],\n",
       "          [-0.1165, -0.0607,  0.0381, -0.1053, -0.0414],\n",
       "          [ 0.1569, -0.1830,  0.0941,  0.0373,  0.1240],\n",
       "          [-0.0495, -0.1071, -0.1234,  0.1242,  0.0520],\n",
       "          [ 0.1945, -0.0309,  0.0649,  0.0143, -0.0539]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1584, -0.0675,  0.0900,  0.0943, -0.1044],\n",
       "          [-0.0956, -0.0699, -0.1691, -0.1267,  0.1515],\n",
       "          [ 0.0864, -0.0489, -0.0379,  0.0264,  0.0268],\n",
       "          [ 0.0410, -0.1575,  0.1524,  0.0815,  0.0208],\n",
       "          [ 0.0557,  0.0477, -0.0418,  0.0830,  0.0777]]],\n",
       "\n",
       "\n",
       "        [[[-0.0221, -0.1223, -0.0940, -0.0227,  0.0494],\n",
       "          [ 0.1191,  0.1469,  0.1449, -0.1497, -0.0680],\n",
       "          [-0.0684, -0.0455,  0.0701, -0.1364,  0.1926],\n",
       "          [-0.1175, -0.1825,  0.1542,  0.0147,  0.0329],\n",
       "          [ 0.0985,  0.1286,  0.0511, -0.1165,  0.1313]]]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e77387b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0257, -0.1003,  0.3286,  0.4681, -0.1894],\n",
       "          [-0.1451,  0.1371, -0.1216,  0.4228, -0.1207],\n",
       "          [ 0.0255, -0.0569, -0.0945, -0.0432, -0.1239],\n",
       "          [ 0.1142, -0.1354,  0.0706,  0.0328,  0.0506],\n",
       "          [ 0.1053,  0.2175,  0.0425, -0.0424, -0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0859,  0.1653,  0.2484,  0.2368, -0.0686],\n",
       "          [ 0.0818,  0.1169, -0.1984, -0.1115, -0.0978],\n",
       "          [-0.0162,  0.0448,  0.1337,  0.0938,  0.2012],\n",
       "          [-0.0523,  0.0512, -0.1359, -0.1301, -0.0707],\n",
       "          [-0.0996, -0.0348,  0.1467,  0.0944, -0.0671]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376, -0.0479, -0.1037, -0.0086,  0.0558],\n",
       "          [ 0.0264,  0.0793,  0.1925, -0.3262, -0.0777],\n",
       "          [ 0.0345,  0.1169, -0.2020,  0.1778,  0.2321],\n",
       "          [ 0.2132, -0.0024,  0.0219, -0.1180, -0.0346],\n",
       "          [ 0.2287,  0.2257, -0.0916,  0.0153, -0.0708]]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a4e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.encoder=model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7963fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0257, -0.1003,  0.3286,  0.4681, -0.1894],\n",
       "          [-0.1451,  0.1371, -0.1216,  0.4228, -0.1207],\n",
       "          [ 0.0255, -0.0569, -0.0945, -0.0432, -0.1239],\n",
       "          [ 0.1142, -0.1354,  0.0706,  0.0328,  0.0506],\n",
       "          [ 0.1053,  0.2175,  0.0425, -0.0424, -0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0859,  0.1653,  0.2484,  0.2368, -0.0686],\n",
       "          [ 0.0818,  0.1169, -0.1984, -0.1115, -0.0978],\n",
       "          [-0.0162,  0.0448,  0.1337,  0.0938,  0.2012],\n",
       "          [-0.0523,  0.0512, -0.1359, -0.1301, -0.0707],\n",
       "          [-0.0996, -0.0348,  0.1467,  0.0944, -0.0671]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376, -0.0479, -0.1037, -0.0086,  0.0558],\n",
       "          [ 0.0264,  0.0793,  0.1925, -0.3262, -0.0777],\n",
       "          [ 0.0345,  0.1169, -0.2020,  0.1778,  0.2321],\n",
       "          [ 0.2132, -0.0024,  0.0219, -0.1180, -0.0346],\n",
       "          [ 0.2287,  0.2257, -0.0916,  0.0153, -0.0708]]]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f117486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in classification_model.encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69adadee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_999270/3496461899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_layer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytts/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "classification_model.encoder.conv2d_layer_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0dfb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0257, -0.1003,  0.3286,  0.4681, -0.1894],\n",
       "          [-0.1451,  0.1371, -0.1216,  0.4228, -0.1207],\n",
       "          [ 0.0255, -0.0569, -0.0945, -0.0432, -0.1239],\n",
       "          [ 0.1142, -0.1354,  0.0706,  0.0328,  0.0506],\n",
       "          [ 0.1053,  0.2175,  0.0425, -0.0424, -0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0859,  0.1653,  0.2484,  0.2368, -0.0686],\n",
       "          [ 0.0818,  0.1169, -0.1984, -0.1115, -0.0978],\n",
       "          [-0.0162,  0.0448,  0.1337,  0.0938,  0.2012],\n",
       "          [-0.0523,  0.0512, -0.1359, -0.1301, -0.0707],\n",
       "          [-0.0996, -0.0348,  0.1467,  0.0944, -0.0671]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376, -0.0479, -0.1037, -0.0086,  0.0558],\n",
       "          [ 0.0264,  0.0793,  0.1925, -0.3262, -0.0777],\n",
       "          [ 0.0345,  0.1169, -0.2020,  0.1778,  0.2321],\n",
       "          [ 0.2132, -0.0024,  0.0219, -0.1180, -0.0346],\n",
       "          [ 0.2287,  0.2257, -0.0916,  0.0153, -0.0708]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f694aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
